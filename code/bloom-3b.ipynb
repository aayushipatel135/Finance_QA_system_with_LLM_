{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6233908,"sourceType":"datasetVersion","datasetId":3524554},{"sourceId":6880372,"sourceType":"datasetVersion","datasetId":3953159}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-11T08:14:17.282304Z","iopub.execute_input":"2023-11-11T08:14:17.282571Z","iopub.status.idle":"2023-11-11T08:14:17.634776Z","shell.execute_reply.started":"2023-11-11T08:14:17.282545Z","shell.execute_reply":"2023-11-11T08:14:17.633737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/finance-mcq-question/finance_MCQ_topic.csv\")\ndf ","metadata":{"execution":{"iopub.status.busy":"2023-11-11T08:14:17.636774Z","iopub.execute_input":"2023-11-11T08:14:17.637288Z","iopub.status.idle":"2023-11-11T08:14:17.692805Z","shell.execute_reply.started":"2023-11-11T08:14:17.637251Z","shell.execute_reply":"2023-11-11T08:14:17.691783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install -q -U bitsandbytes\n! pip install -q -U git+https://github.com/huggingface/transformers.git \n! pip install -q -U git+https://github.com/huggingface/peft.git\n#pip install -q -U git+https://github.com/huggingface/accelerate.git\n#current version of Accelerate on GitHub breaks QLoRa\n#Using standard pip instead\n! pip install -q -U accelerate\n! pip install -q -U datasets","metadata":{"execution":{"iopub.status.busy":"2023-11-11T08:14:17.694746Z","iopub.execute_input":"2023-11-11T08:14:17.695119Z","iopub.status.idle":"2023-11-11T08:16:17.006725Z","shell.execute_reply.started":"2023-11-11T08:14:17.695080Z","shell.execute_reply":"2023-11-11T08:16:17.005492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndata = load_dataset(\"csv\", data_files=\"/kaggle/input/finance-mcq-question/finance_MCQ_topic.csv\")\ndata #= data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-04T15:22:24.852661Z","iopub.execute_input":"2023-11-04T15:22:24.852969Z","iopub.status.idle":"2023-11-04T15:22:26.049472Z","shell.execute_reply.started":"2023-11-04T15:22:24.852943Z","shell.execute_reply":"2023-11-04T15:22:26.048566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig","metadata":{"execution":{"iopub.status.busy":"2023-11-11T08:16:17.009213Z","iopub.execute_input":"2023-11-11T08:16:17.009560Z","iopub.status.idle":"2023-11-11T08:16:22.602531Z","shell.execute_reply.started":"2023-11-11T08:16:17.009513Z","shell.execute_reply":"2023-11-11T08:16:22.601251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"bigscience/bloom-3b\"\n#Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T08:16:22.603946Z","iopub.execute_input":"2023-11-11T08:16:22.604591Z","iopub.status.idle":"2023-11-11T08:16:24.116612Z","shell.execute_reply.started":"2023-11-11T08:16:22.604544Z","shell.execute_reply":"2023-11-11T08:16:24.115615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T08:16:24.117988Z","iopub.execute_input":"2023-11-11T08:16:24.118337Z","iopub.status.idle":"2023-11-11T08:16:24.125062Z","shell.execute_reply.started":"2023-11-11T08:16:24.118308Z","shell.execute_reply":"2023-11-11T08:16:24.123799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config, device_map={\"\":0})","metadata":{"execution":{"iopub.status.busy":"2023-11-11T08:16:24.126685Z","iopub.execute_input":"2023-11-11T08:16:24.126953Z","iopub.status.idle":"2023-11-11T08:17:03.438813Z","shell.execute_reply.started":"2023-11-11T08:16:24.126929Z","shell.execute_reply":"2023-11-11T08:17:03.437940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_prompt(data_point):\n    return f\"\"\" Examine the given options thoroughly and determine the optimal choice that accurately addresses the following multiple-choice question.\n    ### Question:\n    {data_point['Question']}\n    ### Option A:\n    {data_point['Option A']}\n    ### Option B:\n    {data_point['Option B']}\n    ### Option C:\n    {data_point['Option C']}\n    ### Option D:\n    {data_point['Option D']}\n    ### Correct Ans of the above MCQ question is :\n    {data_point['Answer']}\n    ### Above answer is correct because : \n    {data_point['Explanation']}\"\"\"\n\nmapped_qa_dataset = data.map(lambda samples: tokenizer(generate_prompt(samples)))","metadata":{"execution":{"iopub.status.busy":"2023-11-04T15:23:13.470946Z","iopub.execute_input":"2023-11-04T15:23:13.471643Z","iopub.status.idle":"2023-11-04T15:23:25.472079Z","shell.execute_reply.started":"2023-11-04T15:23:13.471603Z","shell.execute_reply":"2023-11-04T15:23:25.471006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n\nmodel = prepare_model_for_kbit_training(model)\n\nconfig = LoraConfig(\n    r=16, \n    lora_alpha=64, \n    target_modules=[\"query_key_value\"], \n    lora_dropout=0.05, #\n    bias=\"none\", \n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, config)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T08:17:03.441160Z","iopub.execute_input":"2023-11-11T08:17:03.441606Z","iopub.status.idle":"2023-11-11T08:17:03.627817Z","shell.execute_reply.started":"2023-11-11T08:17:03.441580Z","shell.execute_reply":"2023-11-11T08:17:03.626932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install wandb","metadata":{"execution":{"iopub.status.busy":"2023-11-04T15:23:25.625036Z","iopub.execute_input":"2023-11-04T15:23:25.625320Z","iopub.status.idle":"2023-11-04T15:23:37.114340Z","shell.execute_reply.started":"2023-11-04T15:23:25.625296Z","shell.execute_reply":"2023-11-04T15:23:37.113065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\n\ntokenizer.pad_token = tokenizer.eos_token\n\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=mapped_qa_dataset[\"train\"],\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=8,\n        num_train_epochs=5,\n        warmup_steps=100,\n        max_steps=1000,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=1,\n        output_dir=\"outputs\",\n        optim=\"paged_adamw_8bit\"\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-11-04T15:23:39.863105Z","iopub.execute_input":"2023-11-04T15:23:39.863484Z","iopub.status.idle":"2023-11-04T22:49:42.099009Z","shell.execute_reply.started":"2023-11-04T15:23:39.863455Z","shell.execute_reply":"2023-11-04T22:49:42.098041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HUGGING_FACE_USER_NAME = \"aayushi135\"","metadata":{"execution":{"iopub.status.busy":"2023-11-11T08:37:37.785796Z","iopub.execute_input":"2023-11-11T08:37:37.786221Z","iopub.status.idle":"2023-11-11T08:37:37.790821Z","shell.execute_reply.started":"2023-11-11T08:37:37.786188Z","shell.execute_reply":"2023-11-11T08:37:37.789801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"bloom3b_finetuned_1000\"","metadata":{"execution":{"iopub.status.busy":"2023-11-11T08:37:37.995799Z","iopub.execute_input":"2023-11-11T08:37:37.996168Z","iopub.status.idle":"2023-11-11T08:37:38.000827Z","shell.execute_reply.started":"2023-11-11T08:37:37.996137Z","shell.execute_reply":"2023-11-11T08:37:37.999596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-11-11T08:38:24.698219Z","iopub.execute_input":"2023-11-11T08:38:24.699163Z","iopub.status.idle":"2023-11-11T08:38:24.735178Z","shell.execute_reply.started":"2023-11-11T08:38:24.699122Z","shell.execute_reply":"2023-11-11T08:38:24.734246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.push_to_hub(f\"{HUGGING_FACE_USER_NAME}/{model_name}\", use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-04T22:51:45.857403Z","iopub.execute_input":"2023-11-04T22:51:45.857778Z","iopub.status.idle":"2023-11-04T22:51:47.808529Z","shell.execute_reply.started":"2023-11-04T22:51:45.857746Z","shell.execute_reply":"2023-11-04T22:51:47.807535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig\nfrom transformers import AutoModelForCausalLM\nfrom peft import PeftModel\nimport torch\n\nbase_model_name_or_path = \"bigscience/bloom-3b\"\nrepo_name = f\"{HUGGING_FACE_USER_NAME}/{model_name}\"\n\nconfig = LoraConfig.from_pretrained(repo_name)\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    config.base_model_name_or_path,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n)\n# Load the LoRA model\ninference_model = PeftModel.from_pretrained(model, repo_name)   # <-- e","metadata":{"execution":{"iopub.status.busy":"2023-11-11T08:38:34.908815Z","iopub.execute_input":"2023-11-11T08:38:34.909828Z","iopub.status.idle":"2023-11-11T08:38:47.438031Z","shell.execute_reply.started":"2023-11-11T08:38:34.909790Z","shell.execute_reply":"2023-11-11T08:38:47.437237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T08:38:47.439563Z","iopub.execute_input":"2023-11-11T08:38:47.439862Z","iopub.status.idle":"2023-11-11T08:38:48.106346Z","shell.execute_reply.started":"2023-11-11T08:38:47.439837Z","shell.execute_reply":"2023-11-11T08:38:48.105451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context = \"Answer the following question\"\nquestion = \"What is a stock market?\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_inference(data_point):\n    batch = tokenizer(f\"\"\"\n    ### Question:\n    {data_point['Questions']}\n    ### Option A:\n    {data_point['Option A']}\n    ### Option B:\n    {data_point['Option B']}\n    ### Option C:\n    {data_point['Option C']}\n    ### Option D:\n    {data_point['Option D']}\n    ### Correct Ans:\"\"\", return_tensors='pt')\n    with torch.cuda.amp.autocast():\n        output_tokens = inference_model.generate(**batch, max_new_tokens=200)\n    print((tokenizer.decode(output_tokens[0], skip_special_tokens=True)))\n    #display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T08:38:52.599453Z","iopub.execute_input":"2023-11-11T08:38:52.600401Z","iopub.status.idle":"2023-11-11T08:38:52.606616Z","shell.execute_reply.started":"2023-11-11T08:38:52.600362Z","shell.execute_reply":"2023-11-11T08:38:52.605626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = pd.read_csv(\"/kaggle/input/finance/test.csv\")\ndf1","metadata":{"execution":{"iopub.status.busy":"2023-11-06T05:23:19.067753Z","iopub.execute_input":"2023-11-06T05:23:19.068647Z","iopub.status.idle":"2023-11-06T05:23:19.094035Z","shell.execute_reply.started":"2023-11-06T05:23:19.068612Z","shell.execute_reply":"2023-11-06T05:23:19.092918Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = pd.read_csv(\"/kaggle/input/finance/CA_exam.csv\")\ndf1.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-11T09:01:01.477777Z","iopub.execute_input":"2023-11-11T09:01:01.478209Z","iopub.status.idle":"2023-11-11T09:01:01.517749Z","shell.execute_reply.started":"2023-11-11T09:01:01.478175Z","shell.execute_reply":"2023-11-11T09:01:01.516616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = []\nfor i in range(0,len(df1))  :\n  print(i)\n  l.append(make_inference(df1.iloc[i,:]))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T09:03:14.878763Z","iopub.execute_input":"2023-11-11T09:03:14.879497Z","iopub.status.idle":"2023-11-11T10:39:01.678432Z","shell.execute_reply.started":"2023-11-11T09:03:14.879447Z","shell.execute_reply":"2023-11-11T10:39:01.677336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = []\nfor i in range(50,len(df1))  :\n  print(i)\n  l.append(make_inference(df1.iloc[i,:]))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T08:59:55.936780Z","iopub.execute_input":"2023-11-11T08:59:55.937256Z","iopub.status.idle":"2023-11-11T08:59:55.942229Z","shell.execute_reply.started":"2023-11-11T08:59:55.937220Z","shell.execute_reply":"2023-11-11T08:59:55.941321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.iloc[0,0]","metadata":{"execution":{"iopub.status.busy":"2023-11-06T05:23:19.534227Z","iopub.execute_input":"2023-11-06T05:23:19.534558Z","iopub.status.idle":"2023-11-06T05:23:19.540944Z","shell.execute_reply.started":"2023-11-06T05:23:19.534530Z","shell.execute_reply":"2023-11-06T05:23:19.540058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = []\nfor i in range(0,50)  :\n  print(i)\n  l.append(make_inference(df1.iloc[i,:]))","metadata":{"execution":{"iopub.status.busy":"2023-11-06T05:23:22.707739Z","iopub.execute_input":"2023-11-06T05:23:22.708730Z","iopub.status.idle":"2023-11-06T05:39:53.107595Z","shell.execute_reply.started":"2023-11-06T05:23:22.708693Z","shell.execute_reply":"2023-11-06T05:39:53.106538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question = df1.iloc[2,1]\nprint(question)\nmake_inference(question)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question = df1.iloc[1,1]\nprint(question)\nmake_inference(question)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_inference2(question):\n    batch = tokenizer(f\"### Instruction:find out the sentiment for following sentence only in one word do not generate any other examples\\n\\n### Input:{question}\\n\\n### Response\\n\", return_tensors='pt')\n    with torch.cuda.amp.autocast():\n        output_tokens = model.generate(**batch, max_new_tokens=200)\n    print((tokenizer.decode(output_tokens[0], skip_special_tokens=True)))\n    #display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question = df1.iloc[0,1]\nprint(question)\nmake_inference2(question)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question = \"Our Company is not getting any work\"\nprint(question)\nmake_inference(question)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_inference1(question):\n    batch = tokenizer(f\"### Instruction:Answer the following question \\n\\n### Input:{question}\\n\\n### Response\\n\", return_tensors='pt')\n    with torch.cuda.amp.autocast():\n        output_tokens = model.generate(**batch, max_new_tokens=200)\n    print((tokenizer.decode(output_tokens[0], skip_special_tokens=True)))\n    #display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))\n    \nquestion = \"What is stock market ?\"\nmake_inference1(question)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(0,100) :\n    print(i)\n    df1.iloc[i,3] = make_inference(df1.iloc[i,1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = pd.read_csv(\"/kaggle/input/finance/sentiment.csv\")\ndf1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1['pred'] = df1['1']\ndf1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = df1.iloc[2,1]\nresult_length = 50\ninputs = tokenizer(prompt, return_tensors=\"pt\")\nprint(tokenizer.decode(model.generate(inputs[\"input_ids\"], \n                       max_length=result_length\n                      )[0]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(0,100) :\n    print(i)\n    prompt = \"Generate the sentiment for following sentence : \" + df1.iloc[i,1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = []\nfor i in range(0,100) :\n    print(i)\n    prompt = \"Generate the sentiment for following sentence : \" + df1.iloc[i,1]\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    l.append(tokenizer.decode(model.generate(inputs[\"input_ids\"])[0]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1['pred'] = df['0'].apply(lambda x : make_inference(x))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.to_csv(\"bloom_3b_generated.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = f\"\"\"### Instruction:\\n{context}\\n\\n### Input:\\n{question} ### Response:\\n\"\"\"\n#device = \"cuda:0\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(device)\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HUGGING_FACE_USER_NAME = \"aayushi135\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"qlora_finance\"\n\nmodel.push_to_hub(f\"{HUGGING_FACE_USER_NAME}/{model_name}\", use_auth_token=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}